// Added to solve .Path problem[{"authors":["admin"],"categories":null,"content":"I am an Assistant Professor in the @Large Research group at the Vrije Universiteit Amsterdam, working on Multiscale and Post-Moore Architectures for Distributed Computer Ecosystems. Before, I was a PostDoc at the SPCL Group at ETH Zurich, working on reconfigurable hardware for High-Performance Computing. I received my MSc and Ph.D. from the University of Pisa.\nMy principal research interests are related to Parallel and Distributed Computing with a particular focus on Dataflow Accelerators (e.g., FPGAs) for HPC, Data Stream Processing, and Energy Awareness in Parallel Computing. In approaching these topics my main objective is to provide the application programmer with high-level abstractions and tools to develop complex parallel software with reduced time-to-market.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an Assistant Professor in the @Large Research group at the Vrije Universiteit Amsterdam, working on Multiscale and Post-Moore Architectures for Distributed Computer Ecosystems. Before, I was a PostDoc at the SPCL Group at ETH Zurich, working on reconfigurable hardware for High-Performance Computing. I received my MSc and Ph.D. from the University of Pisa.\nMy principal research interests are related to Parallel and Distributed Computing with a particular focus on Dataflow Accelerators (e.","tags":null,"title":"Tiziano De Matteis","type":"authors"},{"authors":["Sacheendra Talluri","Nikolas Herbst","Cristina Abad","Tiziano De Matteis","Alexandru Iosup"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713259054,"objectID":"b6a8ca5d087dd546810c0cf066a15bba","permalink":"/publication/stalluri-exde/","publishdate":"2024-04-16T09:17:33.868746Z","relpermalink":"/publication/stalluri-exde/","section":"publication","summary":"Serverless computing is increasingly used for data-processing applications in both science and business domains. At the core of serverless data-processing systems is the scheduler, which ensures dynamic decisions about task and data placement. Due to the variety of user, cluster, and workload properties, the design space for high-performance and cost-effective scheduling architectures and mechanisms is vast. The large design space is difficult to explore and characterize. To help the system designer disentangle this complexity, we present ExDe, a framework to systematically explore the design space of scheduling architectures and mechanisms. The framework includes a conceptual model and a simulator to assist in design space exploration. We use the framework, and real-world workloads, to characterize the performance of three scheduling architectures and two mechanisms. Our framework is open-source software available on Zenodo.","tags":["Serverless","Scheduler","Design","Mechanism","Architecture","Performance"],"title":"ExDe: Design space exploration of scheduler architectures and mechanisms for serverless data-processing","type":"publication"},{"authors":["Dante Niewenhuis","Sacheendra Talluri","Alexandru Iosup","Tiziano de Matteis"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713258517,"objectID":"df65b885051e9f52a24e73be9f6ff344","permalink":"/publication/dniewenhuis-hotcloud-footprinter/","publishdate":"2024-04-16T09:08:37.476022Z","relpermalink":"/publication/dniewenhuis-hotcloud-footprinter/","section":"publication","summary":"Data centers have become an increasingly significant contributor to the global carbon footprint. In 2021, the global data center industry was responsible for around 1% of the worldwide greenhouse gas emissions. With more resource-intensive workloads, such as Large Language Models, gaining popularity, this percentage is expected to increase further. Therefore, it is crucial for data center service providers to become aware of and accountable for the sustainability impact of their design and operational choices. However, reducing the carbon footprint of data centers has been a challenging process due to the lack of comprehensive metrics, carbon-aware design tools, and guidelines for carbon-aware optimization. In this work, we propose FootPrinter, a first-of-its-kind tool that supports data center designers and operators in assessing the environmental impact of their data center. FootPrinter uses coarse-grained operational data, grid energy mix information, and discrete event simulation to determine the data centerâ€™s operational carbon footprint and evaluate the impact of infrastructural or operational changes. FootPrinter can simulate days of operations of a regional data center on a commodity laptop in a few seconds, returning the estimated footprint with marginal error. By making this project open source, we hope to engage the community in the development of methodologies and tools for systematically assessing and exploring the sustainability of data centers.","tags":[],"title":"FootPrinter: Quantifying Data Center Carbon Footprint","type":"publication"},{"authors":["Aratz Manterola Lasa","Sacheendra Talluri","Tiziano De Matteis","Alexandru Iosup"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713258841,"objectID":"af9a387716e0479c277398f3b0aaa311","permalink":"/publication/2024-icpe-datacenter-scheduler/","publishdate":"2024-04-16T09:14:01.671431Z","relpermalink":"/publication/2024-icpe-datacenter-scheduler/","section":"publication","summary":"Schedulers are a crucial component in datacenter resource management. Each scheduler offers different capabilities, and users use them through their APIs. However, there is no clear understanding of what programming abstractions they offer, nor why they offer some and not others. Consequently, it is difficult to understand their differences and the performance costs imposed by their APIs. In this work, we study the programming abstractions offered by industrial schedulers, their shortcomings, and their related performance costs. We propose a general reference architecture for scheduler programming abstractions. Specifically, we analyze the programming abstractions of five popular industrial schedulers, understand the differences in their APIs, and identify the missing abstractions. Finally, we carry out exemplary experiments using trace-driven simulation demonstrating that an API extension, such as container migration, can improve total execution time per task by 81%, highlighting how schedulers sacrifice performance by implementing simpler programming abstractions. All the relevant software and data artifacts are publicly available at https://github. com/atlarge-research/quantifying-api-design.","tags":[],"title":"The Cost of Simplicity: Understanding Datacenter Scheduler Programming Abstractions","type":"publication"},{"authors":["Tiziano De Matteis","Lukas Gianinazzi","Johannes de Fine Licht","Torsten Hoefler"],"categories":[],"content":"","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713258635,"objectID":"f109a94f7f94fbb1cb16c8279392b00e","permalink":"/publication/streaming-scheduling/","publishdate":"2024-04-16T09:10:34.97333Z","relpermalink":"/publication/streaming-scheduling/","section":"publication","summary":"Dataflow devices represent an avenue towards saving the control and data movement overhead of Load-Store Architectures. Various dataflow accelerators have been proposed, but how to efficiently schedule applications on such devices remains an open problem. The programmer can explicitly implement both temporal and spatial parallelism, and pipelining across multiple processing elements can be crucial to take advantage of the fast on-chip interconnect, enabling the concurrent execution of different program components. This paper introduces canonical task graphs, a model that enables streaming scheduling of task graphs over dataflow architectures. We show how a task graph can be statically analyzed to understand its steady-state behavior, and we use this information to partition it into temporally multiplexed components of spatially executed tasks. Results on synthetic and realistic workloads show how streaming scheduling can increase speedup and device utilization over a traditional scheduling approach.","tags":[],"title":"Streaming Task Graph Scheduling for Dataflow Architectures","type":"publication"},{"authors":["Daniele De Sensi","Tiziano De Matteis","Konstantin Taranov","Salvatore Di Girolamo","Tobias Rahn","Torsten Hoefler"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"5aad6a8de69a968be8ff2b896790b5cd","permalink":"/publication/noise-cloud/","publishdate":"2023-03-01T13:44:10.832638Z","relpermalink":"/publication/noise-cloud/","section":"publication","summary":"Cloud computing represents an appealing opportunity for cost-effective deployment of HPC workloads on the best-fitting hardware. However, although cloud and on-premise HPC systems offer similar computational resources, their network architecture and performance may differ significantly. For example, these systems use fundamentally different network transport and routing protocols, which may introduce network noise that can eventually limit the application scaling. This work analyzes network performance, scalability, and cost of running HPC workloads on cloud systems. First, we consider latency, bandwidth, and collective communication patterns in detailed small-scale measurements, and then we simulate network performance at a larger scale. We validate our approach on four popular cloud providers and three on-premise HPC systems, showing that network (and also OS) noise can significantly impact performance and cost both at small and large scale.","tags":null,"title":"Noise in the Clouds: Influence of Network Performance Variability on Application Scalability","type":"publication"},{"authors":["Carl-Johannes Johnsen","Tiziano De Matteis Tal Ben-Nun","Johannes de Fine Licht","Torsten Hoefler"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"2826617a6a9b20b203433a89798e751a","permalink":"/publication/double-pumping/","publishdate":"2022-09-02T07:55:53.906943Z","relpermalink":"/publication/double-pumping/","section":"publication","summary":" The multi-pumping resource sharing technique can overcome the limitations commonly found in single-clocked FPGA designs by allowing hardware components to operate at a higher clock frequency than the surrounding system. However, this optimization cannot be expressed in high levels of abstraction, such as HLS, requiring the use of hand-optimized RTL. In this paper we show how to leverage multiple clock domains for computational subdomains on reconfigurable devices through data movement analysis on high-level programs.  We offer a novel view on multi-pumping as a compiler optimization --- a superclass of traditional vectorization. As multiple data elements are fed and consumed, the computations are packed temporally rather than spatially. The optimization is applied automatically using an intermediate representation that maps high-level code to HLS. Internally, the optimization injects modules into the generated designs, incorporating RTL for fine-grained control over the clock domains. We obtain a reduction of resource consumption by up to 50% on critical components and 23% on average. For scalable designs, this can enable further parallelism, increasing overall performance. ","tags":null,"title":"Temporal Vectorization: A Compiler Approach to Automatic Multi-Pumping","type":"publication"},{"authors":["Johannes de Fine Licht","Tiziano De Matteis","Tal Ben-Nun","Andreas Kuster","Oliver Rausch","Manuel Burger","Carl-Johannes Johnsen","Torsten Hoefler"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713258605,"objectID":"2bea79af36a9c0901eec48a87fd27001","permalink":"/publication/licht-2022-python/","publishdate":"2024-04-16T09:10:05.522346Z","relpermalink":"/publication/licht-2022-python/","section":"publication","summary":"Although high-level synthesis (HLS) tools have significantly improved programmer productivity over hardware description languages, developing for FPGAs remains tedious and error prone. Programmers must learn and implement a large set of vendor-specific syntax, patterns, and tricks to optimize (or even successfully compile) their applications, while dealing with ever-changing toolflows from the FPGA vendors. We propose a new way to develop, optimize, and compile FPGA programs. The Data-Centric parallel programming (DaCe) framework allows applications to be defined by their dataflow and control flow through the Stateful DataFlow multiGraph (SDFG) representation, capturing the abstract program characteristics, and exposing a plethora of optimization opportunities. In this work, we show how extending SDFGs with multi-level Library Nodes incorporates both domain-specific and platform-specific optimizations into the design flow, enabling knowledge transfer across application domains and FPGA vendors. We present the HLS-based FPGA code generation backend of DaCe, and show how SDFGs are code generated for either FPGA vendor, emitting efficient HLS code that is structured and annotated to implement the desired architecture.","tags":[],"title":"Python FPGA Programming with Data-Centric Multi-Level Design","type":"publication"},{"authors":["Alexandros Nikolaos Ziogas","Timo Schneider","Tal Ben-Nun","Alexandru Calotoiu","Tiziano De Matteis","Johannes de Fine Licht","Luca Lavarini","Torsten Hoefler"],"categories":[],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634637805,"objectID":"dd73875bcb02f6bf0a1a0025ee6833c9","permalink":"/publication/data-centric-python/","publishdate":"2021-10-19T10:03:24.99107Z","relpermalink":"/publication/data-centric-python/","section":"publication","summary":"Python has become the de facto language for scientific computing. Programming in Python is highly productive, mainly due to its rich science-oriented software ecosystem built around the NumPy module. As a result, the demand for Python support in High Performance Computing (HPC) has skyrocketed. However, the Python language itself does not necessarily offer high performance. In this work, we present a workflow that retains Python's high productivity while achieving portable performance across different architectures. The workflow's key features are HPC-oriented language extensions and a set of automatic optimizations powered by a data-centric intermediate representation. We show performance results and scaling across CPU, GPU, FPGA, and the Piz Daint supercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over previous-best solutions, first-ever Xilinx and Intel FPGA results of annotated Python, and up to 93.16% scaling efficiency on 512 nodes.","tags":[],"title":"Productivity, Portability, Performance: Data-Centric Python","type":"publication"},{"authors":["Johannes de Fine Licht","Andreas Kuster","Tiziano De Matteis","Tal Ben-Nun","Dominic Hofer","Torsten Hoefler"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"e00f8c5d3578ee4e86440f7d9de41a5b","permalink":"/publication/stencilflow/","publishdate":"2021-02-18T14:22:23.873835Z","relpermalink":"/publication/stencilflow/","section":"publication","summary":"Spatial computing devices have been shown to significantly accelerate stencil computations, but have so far relied on unrolling the iterative dimension of a single stencil operation to increase temporal locality. This work considers the general case of mapping directed acyclic graphs of heterogeneous stencil computations to spatial computing systems, assuming large input programs without an iterative component. StencilFlow maximizes temporal locality and ensures deadlock freedom in this setting, providing end-to-end analysis and mapping from a high-level program description to distributed hardware. We evaluate our generated architectures on a Stratix 10 FPGA testbed, yielding 1.31 TOp/s and 4.18 TOp/s on single-device and multi-device, respectively, demonstrating the highest performance recorded for stencil programs on FPGAs to date. We then leverage the framework to study a complex stencil program from a production weather simulation application. Our work enables productively targeting distributed spatial computing systems with large stencil programs, and offers insight into architecture characteristics required for their efficient execution in practice.","tags":null,"title":"StencilFlow: Mapping Large Stencil Programs to Distributed Spatial Computing Systems","type":"publication"},{"authors":["Tiziano De Matteis","Johannes de Fine Licht","and Torsten Hoefler"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"bbecb3dc55ce7402ccd1073da16536e5","permalink":"/publication/fblas/","publishdate":"2021-02-18T14:22:23.874249Z","relpermalink":"/publication/fblas/","section":"publication","summary":"Spatial computing architectures pose an attractive alternative to mitigate control and data movement overheads typical of load-store architectures. In practice, these devices are rarely considered in the HPC community due to the steep learning curve, low productivity, and the lack of available libraries for fundamental operations. High-level synthesis (HLS) tools are facilitating hardware programming, but optimizing for these architectures requires factoring in new transformations and resources/performance trade-offs. We present FBLAS, an open-source HLS implementation of BLAS for FPGAs, that enables reusability, portability and easy integration with existing software and hardware codes. FBLAS' implementation allows scaling hardware modules to exploit on-chip resources, and module interfaces are designed to natively support streaming on-chip communications, allowing them to be composed to reduce off-chip communication. With FBLAS, we set a precedent for FPGA library design, and contribute to the toolbox of customizable hardware components necessary for HPC codes to start productively targeting reconfigurable platforms.","tags":null,"title":" FBLAS: Streaming Linear Algebra on FPGA","type":"publication"},{"authors":["Tiziano De Matteis","Johannes de Fine Licht","Jakub BerÃ¡nek","Torsten Hoefler"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"99664b91eb5be80a18a57472d7962559","permalink":"/publication/smi/","publishdate":"2021-02-18T14:22:23.876127Z","relpermalink":"/publication/smi/","section":"publication","summary":"Distributed memory programming is the established paradigm used in high-performance computing (HPC) systems, requiring explicit communication between nodes and devices. When FPGAs are deployed in distributed settings, communication is typically handled either by going through the host machine, sacrificing performance, or by streaming across fixed device-to-device connections, sacrificing flexibility. We present Streaming~Message~Interface~(SMI), a communication model and API that unifies explicit message passing with a hardware-oriented programming model, facilitating minimal-overhead, flexible, and productive inter-FPGA communication. Instead of bulk transmission, messages are streamed across the network during computation, allowing communication to be seamlessly integrated into pipelined designs. We present a high-level synthesis implementation of SMI targeting a dedicated FPGA interconnect, exposing runtime-configurable routing with support for arbitrary network topologies, and implement a set of distributed memory benchmarks. Using SMI, programmers can implement distributed, scalable HPC programs on reconfigurable hardware, without deviating from best practices for hardware design.","tags":null,"title":"Streaming Message Interface: High-Performance DistributedMemory Programming on Reconfigurable Hardware","type":"publication"},{"authors":null,"categories":null,"content":" Streaming Message Interface is a a distributed memory HLS programming model for FPGAs that provides the convenience of message passing for HLS-programmed hardware devices. Instead of bulk transmission, typical of message passing model, with SMI messages are streamed across the network during computation, allowing communication to be seamlessly integrated into pipelined designs.\nThis repository contains an high-level synthesis implementation of SMI targeting OpenCL and Intel FPGAs, and all the applications used for the evaluation perfomed in the paper: \u0026ldquo;Streaming Message Interface: High-Performance Distributed Memory Programming on Reconfigurable Hardware\u0026rdquo;, Tiziano De Matteis, Johannes de Fine Licht, Jakub BerÃ¡nek, and Torsten Hofler. To appear in Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis, 2019 (SC 2019).\nPlease refer to the project webpage and to the paper for a reference on how to use SMI for your own distributed FPGA programs.\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"50bf2468654b1ae0db8ee3c795bd43a3","permalink":"/software/smi-project/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/software/smi-project/","section":"software","summary":"Streaming Message Interface: High-Performance Distributed Memory Programming on Reconfigurable Hardware","tags":["FPGA"],"title":"SMI","type":"software"},{"authors":["Tiziano De Matteis","Gabriele Mencagli","Daniele De Sensi","Massimo Torquati","Marco Danelutto"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"d39667d7021ca0999bdcd08215f02034","permalink":"/publication/gasser/","publishdate":"2021-02-18T14:22:23.876758Z","relpermalink":"/publication/gasser/","section":"publication","summary":"Today's stream processing systems handle high-volume data streams in an efficient manner. To achieve this goal, they are designed to scale out on large clusters of commodity machines. However, despite the efficient use of distributed architectures, they lack support to co-processors like graphical processing units (GPUs) ready to accelerate data-parallel tasks. The main reason for this lack of integration is that GPU processing and the streaming paradigm have different processing models, with GPUs needing a bulk of data present at once while the streaming paradigm advocates a tuple-at-a-time processing model. This paper contributes to fill this gap by proposing Gasser, a system for offloading the execution of sliding-window operators on GPUs. The system focuses on completely general functions by targeting the parallel processing of non-incremental queries that are not supported by the few existing GPU-based streaming prototypes. Furthermore, Gasser provides an auto-tuning approach able to automatically find the optimal value of the configuration parameters (i.e., batch length and the degree of parallelism) needed to optimize throughput and latency with the given query and data stream. The experimental part assesses the performance efficiency of Gasser by comparing its peak throughput and latency against Apache Flink, a popular and scalable streaming system. Furthermore, we evaluate the penalty induced by supporting completely general queries against the performance achieved by the state-of-the-art solution specifically optimized for incremental queries. Finally, we show the speed and accuracy of the auto-tuning approach adopted by Gasser, which is able to self-configure the system by finding the right configuration parameters without manual tuning by the users.","tags":null,"title":"GASSER: An Auto-Tunable System for General Sliding-Window Streaming Operators on GPUs","type":"publication"},{"authors":null,"categories":null,"content":" FBLAS is a porting of the BLAS numerical library (http://www.netlib.org/blas/) for Intel FPGA platform.\nFBLAS provides two layers of abstraction:\nHLS modules, which can be integrated into existing hardware designs. They implement BLAS routines (DOT, GEMV, GEMM, etc.). Modules have been designed with compute performance in mind, exploiting the spatial parallelism and fast on-chip memory on FPGAs and have a streaming interface: data is received and produced using channels. In this way, they can be composed and communicate using on-chip resources rather than off-chip device RAM;\na high-level Host API conforming to the classical BLAS interface that allows the user to invoke routines directly from a host program. No prior knowledge on FPGA architecture and/or tools is needed. The user writes a standard OpenCL program: she is responsible to transferring data to and from the device, she can invoke the desired FBLAS routines working on the FPGA memory, and then she copies back the result from the device.\nFor further information on how to use the library, please refer to the project page.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"92e8fce1c7260723f54cdae45adf1bc3","permalink":"/software/fblas-project/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/software/fblas-project/","section":"software","summary":"BLAS implementation for Intel FPGA","tags":["FPGA"],"title":"FBLAS","type":"software"},{"authors":["Alessio Conte","Tiziano De Matteis","Daniele De Sensi","Roberto Grossi","Andrea Marino","Luca Versari"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"65a83033d6382e7eaa6d5cadb4b38895","permalink":"/publication/kdd-18/","publishdate":"2021-02-18T14:22:23.878Z","relpermalink":"/publication/kdd-18/","section":"publication","summary":"This paper studies kplexes, a well known pseudo-clique model for network communities. In a kplex, each node can miss at most $k-1$ links. Our goal is to detect large communities in today's real-world graphs which can have hundreds of millions of edges. While many have tried, this task has been elusive so far due to its computationally challenging nature: kplexes and other pseudo-cliques are harder to find and more numerous than cliques, a well known hard problem. We present D2K, which is the first algorithm able to find large kplexes of very large graphs in just a few minutes. The good performance of our algorithm follows from a combination of graph-theoretical concepts, careful algorithm engineering and a high-performance implementation. In particular, we exploit the low degeneracy of real-world graphs, and the fact that large enough kplexes have diameter~2. We validate a sequential and a parallel/distributed implementation of D2K on real graphs with up to half a billion edges.","tags":["community discovery","graph enumeration","k-plexes","parallel programming"],"title":"D2K: Scalable Community Detection in Massive Networks via Small-Diameter k-Plexes","type":"publication"},{"authors":["Massimo Torquati","Tullio Menga","Tiziano De Matteis","Daniele De Sensi","Gabriele Mencagli"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"cc75b1949b9429f44ae6e0fed40dda56","permalink":"/publication/cafpdp-18/","publishdate":"2021-02-18T14:22:23.878937Z","relpermalink":"/publication/cafpdp-18/","section":"publication","summary":"In this work, we consider the C++ Actor Framework (CAF), a recent proposal that revamped the interest in building concurrent and distributed applications using the actor programming model in C++. CAF has been optimized for high-throughput computing, whereas message latency between actors is greatly influenced by the message data rate: at low and moderate rates the latency is higher than at high data rates. To this end, we propose a modification of the polling strategies in the work-stealing CAF scheduler, which can reduce message latency at low and moderate data rates up to two orders of magnitude without compromising the overall throughput and message latency at maximum pressure. The technique proposed uses a lightweight event notification protocol that is general enough to be used used to optimize the runtime of other frameworks experiencing similar issues.","tags":null,"title":"Reducing Message Latency and CPU Utilization in the CAF Actor Framework","type":"publication"},{"authors":["Daniele De Sensi","Tiziano De Matteis","Marco Danelutto"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5ce69e053c4b9f4770ec1c0327514ebd","permalink":"/publication/nornir-fgcs-18/","publishdate":"2021-02-18T14:22:23.878325Z","relpermalink":"/publication/nornir-fgcs-18/","section":"publication","summary":"Self-adaptation is an emerging requirement in parallel computing. It enables the dynamic selection of resources toallocate to the application in order to meet performance and power consumption requirements. This is particularly relevant in Fog Applications, where data is generated by a number of devices at a varying rate, according to usersâ€™ activity. By dynamically selecting the appropriate number of resources it is possible, for example, to use at each time step the minimum amount of resources needed to process the incoming data. Implementing such kind of algorithms may be a complex task, due to low-level interactions with the underlying hardware and to non-intrusive and low-overhead monitoring of the applications. For these reasons, in this paper we propose Nornir, a C++-based framework, which can be used to enforce performance and power consumption constraints on parallel applications running on shared memory multicores. The framework can be easily customized by algorithm designers to implement new self-adaptive policies. By instrumenting the applications in the PARSEC benchmark, we provide to strategy designers a wide set of applications already interfaced to Nornir. In addition to this, to prove its flexibility, we implemented and compared several state-of-the-art existing policies, showing that Nornir can also be used to easily analyze different algorithms and to provide useful insights on them.","tags":["Self-adaptive","Power-aware","Quality of service","Data stream processing","Fog computing","Parallel computing"],"title":"Simplifying self-adaptive and power-aware computing with Nornir","type":"publication"},{"authors":["Marco Danelutto","Tiziano De Matteis","Daniele De Sensi","Gabriele Mencagli","Massimo Torquati","Marco Aldinucci","Peter Kilpatrick"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"ed7dcb03698cb92156a44ca2e0db2696","permalink":"/publication/rephrase-ijpp-17/","publishdate":"2021-02-18T14:22:23.888681Z","relpermalink":"/publication/rephrase-ijpp-17/","section":"publication","summary":"We discuss the extended parallel pattern set identified within the EU-funded project RePhrase as a candidate pattern set to support data intensive applications targeting heterogeneous architectures. The set has been designed to include three classes of pattern, namely (1) core patterns, modelling common, not necessarily data intensive parallelism exploitation patterns, usually to be used in composition; (2) high level patterns, modelling common, complex and complete parallelism exploitation patterns; and (3) building block patterns, modelling the single components of data intensive applications, suitable for use---in composition---to implement patterns not covered by the core and high level patterns. We discuss the expressive power of the RePhrase extended pattern set and results illustrating the performances that may be achieved with the FastFlow implementation of the high level patterns.","tags":null,"title":"The RePhrase Extended Pattern Set for Data Intensive Parallel Computing","type":"publication"},{"authors":["Daniele De Sensi","Tiziano De Matteis","Massimo Torquati","Gabriele Mencagli","Marco Danelutto"],"categories":null,"content":"","date":1506816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506816000,"objectID":"5b82308b71d5cbf5ae8dee9c17cd4b9b","permalink":"/publication/p-3-arsec-taco-17/","publishdate":"2021-02-18T14:22:23.889308Z","relpermalink":"/publication/p-3-arsec-taco-17/","section":"publication","summary":"High-level parallel programming is an active research topic aimed at promoting parallel programming methodologies that provide the programmer with high-level abstractions to develop complex parallel software with reduced time to solution. Pattern-based parallel programming is based on a set of composable and customizable parallel patterns used as basic building blocks in parallel applications. In recent years, a considerable effort has been made in empowering this programming model with features able to overcome shortcomings of early approaches concerning flexibility and performance. In this article, we demonstrate that the approach is flexible and efficient enough by applying it on 12 out of 13 PARSEC applications. Our analysis, conducted on three different multicore architectures, demonstrates that pattern-based parallel programming has reached a good level of maturity, providing comparable results in terms of performance with respect to both other parallel programming methodologies based on pragma-based annotations (i.e., Openmp and OmpSs) and native implementations (i.e., Pthreads). Regarding the programming effort, we also demonstrate a considerable reduction in lines of code and code churn compared to Pthreads and comparable results with respect to other existing implementations.","tags":["Parallel patterns","algorithmic skeletons","benchmarking","multicore programming","parsec"],"title":"Bringing Parallel Patterns Out of the Corner: The P$^3$ARSEC Benchmark Suite","type":"publication"},{"authors":["Massimo Torquati","Gabriele Mencagli","Maurizio Drocco","Marco Aldinucci","Tiziano De Matteis","Marco Danelutto"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"d2b02e0e1f827c00c641741c4591c8ef","permalink":"/publication/jsc-17/","publishdate":"2021-02-18T14:22:23.890856Z","relpermalink":"/publication/jsc-17/","section":"publication","summary":"This work studies the issues related to dynamic memory management in Data Stream Processing, an emerging paradigm enabling the real-time processing of live data streams. In this paper, we consider two streaming parallel patterns and we discuss different implementation variants related to how dynamic memory is managed. The results show that the standard mechanisms provided by modern C++ are not entirely adequate for maximizing the performance. Instead, the combined use of an efficient general purpose memory allocator, a custom allocator optimized for the pattern considered and a custom variant of the C++ shared pointer mechanism, provides a performance improvement up to 16% on the best case.","tags":null,"title":"On dynamic memory allocation in sliding-window parallel patterns for streaming analytics","type":"publication"},{"authors":["Gabriele Mencagli","Massimo Torquati","Marco Danelutto","Tiziano De Matteis"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"8a45bd2fd24f0d14d6ed7760b9679c85","permalink":"/publication/tpds-17/","publishdate":"2021-02-18T14:22:23.893223Z","relpermalink":"/publication/tpds-17/","section":"publication","summary":"Techniques to handle traffic bursts and out-of-order arrivals are of paramount importance to provide real-time sensor data analytics in domains like traffic surveillance, transportation management, healthcare and security applications. In these systems the amount of raw data coming from sensors must be analyzed by continuous queries that extract value-added information used to make informed decisions in real-time. To perform this task with timing constraints, parallelism must be exploited in the query execution in order to enable the real-time processing on parallel architectures. In this paper we focus on continuous preference queries, a representative class of continuous queries for decision making, and we propose a parallel query model targeting the efficient processing over out-of-order and bursty data streams. We study how to integrate punctuation mechanisms in order to enable out-of-order processing. Then, we present advanced scheduling strategies targeting scenarios with different burstiness levels, parameterized using the index of dispersion quantity. Extensive experiments have been performed using synthetic datasets and real-world data streams obtained from an existing real-time locating system. The experimental evaluation demonstrates the efficiency of our parallel solution and its effectiveness in handling the out-of-orderness degrees and burstiness levels of real-world applications.","tags":null,"title":"Parallel Continuous Preference Queries over Out-of-Order and Bursty Data Streams","type":"publication"},{"authors":["Tiziano De Matteis","Gabriele Mencagli"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"f1ce1bcb63348e3132841aca0741e848","permalink":"/publication/ijpp-17/","publishdate":"2021-02-18T14:22:23.883446Z","relpermalink":"/publication/ijpp-17/","section":"publication","summary":"The topic of Data Stream Processing is a recent and highly active research area dealing with the in-memory, tuple-by-tuple analysis of streaming data. Continuous queries typically consume huge volumes of data received at a great velocity. Solutions that persistently store all the input tuples and then perform off-line computation are impractical. Rather, queries must be executed continuously as data cross the streams. The goal of this paper is to present parallel patterns for window-based stateful operators, which are the most representative class of stateful data stream operators. Parallel patterns are presented ``Ã  la'' Algorithmic Skeleton, by explaining the rationale of each pattern, the preconditions to safely apply it, and the outcome in terms of throughput, latency and memory consumption. The patterns have been implemented in the $$backslashmathtt FastFlow$$ FastFlow framework targeting off-the-shelf multicores. To the best of our knowledge this is the first time that a similar effort to merge the Data Stream Processing domain and the field of Structured Parallelism has been made.","tags":null,"title":"Parallel Patterns for Window-Based Stateful Operators on Data Streams: An Algorithmic Skeleton Approach","type":"publication"},{"authors":["Tiziano De Matteis","Gabriele Mencagli"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"6e5052bf2f09747eb021ef79930f8bd7","permalink":"/publication/dasp-pdp-17/","publishdate":"2021-02-18T14:22:23.880328Z","relpermalink":"/publication/dasp-pdp-17/","section":"publication","summary":"High-volume data streams are straining the limits of stream processing frameworks which need advanced parallel processing capabilities to withstand the actual incoming bandwidth. Parallel processing must be synergically integrated with elastic features in order dynamically scale the amount of utilized resources by accomplishing the Quality of Service goals in a costeffective manner. This paper proposes a control-theoretic strategy to drive the elastic behavior of latency-sensitive streaming operators in distributed environments. The strategy takes scaling decisions in advance by relying on a predictive model-based approach. Our ideas have been experimentally evaluated on a cluster using a real-world streaming application fed by synthetic and real datasets. The results show that our approach takes the strictly necessary reconfigurations while providing reduced resource consumption. Furthermore, it allows the operator to meet desired average latency requirements with a significant reduction in the experienced latency jitter.","tags":null,"title":"Elastic Scaling for Distributed Latency-sensitive Data Stream Operators","type":"publication"},{"authors":["Marco Danelutto","Tiziano De Matteis","Daniele De Sensi","Massimo Torquati"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"ae19811d2130b99e862c68a9f6f30695","permalink":"/publication/pdp-17/","publishdate":"2021-02-18T14:22:23.88109Z","relpermalink":"/publication/pdp-17/","section":"publication","summary":"Power-aware computing is gaining an increasing attention both in academic and industrial settings. The problem of guaranteeing a given QoS requirement (either in terms of performance or power consumption) can be faced by selecting and dynamically adapting the amount of physical and logical resources used by the application. In this study, we considered standard multicore platforms by taking as a reference approaches for power-aware computing two well-known dynamic reconfiguration techniques: Concurrency Throttling and Thread Packing. Furthermore, we also studied the impact of using simultaneous multithreading (e.g., Intelâ€™s HyperThreading) in both techniques. In this work, leveraging on the applications of the PARSEC benchmark suite, we evaluate these techniques by considering performance-power trade-offs, resource efficiency, predictability and required programming effort. The results show that, according to the comparison criteria, these techniques complement each other.","tags":null,"title":"Evaluating Concurrency Throttling and Thread Packing on SMT Multicores","type":"publication"},{"authors":["Daniele De Sensi","Tiziano De Matteis","Marco Danelutto"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"fe1fbf406cbf29451c71aa0c5b184f92","permalink":"/publication/nornir-autodasp-17/","publishdate":"2021-02-18T14:22:23.889953Z","relpermalink":"/publication/nornir-autodasp-17/","section":"publication","summary":"A desirable characteristic of modern parallel applications is the ability to dynamically select the amount of resources to be used to meet requirements on performance or power consumption. In many cases, providing explicit guarantees on performance is of paramount importance. In streaming applications, this is related with the concept of elasticity, i.e. being able to allocate the proper amount of resources to match the current demand as closely as possible. Similarly, in other scenarios, it may be useful to limit the maximum power consumption of an application to do not exceed the available power budget. In this paper we propose Nornir, a customizable C++ framework for autonomic and power-aware parallel applications on shared memory multicore machines. Nornir can be used by autonomic strategy designers to implement new algorithms and by application users to enforce requirements on their applications.","tags":null,"title":"Nornir: A Customisable Framework for Autonomic and Power-Aware Applications","type":"publication"},{"authors":["Marco Danelutto","Tiziano De Matteis","Daniele De Sensi","Gabriele Mencagli","Massimo Torquati"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f3f03193e0a1dd847ef460c159c03926","permalink":"/publication/sac-17/","publishdate":"2021-02-18T14:22:23.879838Z","relpermalink":"/publication/sac-17/","section":"publication","summary":"High-level parallel programming is a de-facto standard approach to develop parallel software with reduced time to development. High-level abstractions are provided by existing frameworks as pragma-based annotations in the source code, or through pre-built parallel patterns that recur frequently in parallel algorithms, and that can be easily instantiated by the programmer to add a structure to the development of parallel software. In this paper we focus on this second approach and we propose P3ARSEC, a benchmark suite for parallel pattern-based frameworks consisting of a representative subset of PARSEC applications. We analyse the programmability advantages and the potential performance penalty of using such high-level methodology with respect to hand-made parallelisations using low-level mechanisms. The results are obtained on the new Intel Knights Landing multicore, and show a significantly reduced code complexity with comparable performance.","tags":["Parallel Patterns","PARSEC Benchmarks","Intel KNL"],"title":"P$^3$ARSEC: Towards Parallel Patterns Benchmarking","type":"publication"},{"authors":["Tiziano De Matteis","Gabriele Mencagli"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3839f31541432d02a18139d7078a4273","permalink":"/publication/jss-17/","publishdate":"2021-02-18T14:22:23.881999Z","relpermalink":"/publication/jss-17/","section":"publication","summary":"Abstract Data stream processing applications have a long running nature (24 hr/7 d) with workload conditions that may exhibit wide variations at run-time. Elasticity is the term coined to describe the capability of applications to change dynamically their resource usage in response to workload fluctuations. This paper focuses on strategies for elastic data stream processing targeting multicore systems. The key idea is to exploit Model Predictive Control, a control-theoretic method that takes into account the system behavior over a future time horizon in order to decide the best reconfiguration to execute. We design a set of energy-aware proactive strategies, optimized for throughput and latency QoS requirements, which regulate the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support offered by modern multicore CPUs. We evaluate our strategies in a high-frequency trading application fed by synthetic and real-world workload traces. We introduce specific properties to effectively compare different elastic approaches, and the results show that our strategies are able to achieve the best outcome. ","tags":["Data stream processing"],"title":"Proactive elasticity and energy awareness in data stream processing ","type":"publication"},{"authors":["Marco Danelutto","Tiziano De Matteis","Gabriele Mencagli","Massimo Torquati"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"39be793610c0065908ccf8b7050dae43","permalink":"/publication/seps-16/","publishdate":"2021-02-18T14:22:23.881544Z","relpermalink":"/publication/seps-16/","section":"publication","summary":"","tags":["Divide and Conquer","High-level parallel patterns"],"title":"A Divide-and-conquer Parallel Pattern Implementation for Multicores","type":"publication"},{"authors":["Tiziano De Matteis","Salvatore Di Girolamo","Gabriele Mencagli"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"e36183f8e7948e6aae82f1bb83b9f220","permalink":"/publication/ccpe-2016/","publishdate":"2021-02-18T14:22:23.882994Z","relpermalink":"/publication/ccpe-2016/","section":"publication","summary":"The emergence of real-time decision-making applications in domains like high-frequency trading, emergency management and service level analysis in communication networks, has led to the definition of new classes of queries. Skyline queries are a notable example. Their results consist of all the tuples whose attribute vector is not dominated (in the Pareto sense) by one of any other tuple. Because of their popularity, skyline queries have been studied in terms of both sequential algorithms and parallel implementations for multiprocessors and clusters. Within the Data Stream Processing paradigm, traditional database queries on static relations have been revised in order to operate on continuous data streams. Most of the past papers propose sequential algorithms for continuous skyline queries, whereas there exist very few works targeting implementations on parallel machines. This paper contributes to fill this gap by proposing a parallel implementation for multicore architectures. We propose: i) a parallelization of the eager algorithm based on the notion of Skyline Influence Time, ii) optimizations of the reduce phase and load-balancing strategies to achieve near-optimal speedup, iii) a set of experiments with both synthetic benchmarks and a real dataset in order to show our implementation effectiveness","tags":null,"title":"Continuous Skyline Queries on Multicore Architectures","type":"publication"},{"authors":["Marco Danelutto","Tiziano De Matteis","Gabriele Mencagli","Massimo Torquati"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"1699182e3be88fed2e15826d287c9a2c","permalink":"/publication/js-2016/","publishdate":"2021-02-18T14:22:23.88256Z","relpermalink":"/publication/js-2016/","section":"publication","summary":"Time-to-solution is an important metric when parallelizing existing code. The REPARA approach provides a systematic way to instantiate stream and data parallel patterns by annotating the sequential source code with Cpp11 attributes. Annotations are automatically transformed in a target parallel code that uses existing libraries for parallel programming (e.g., FastFlow). In this paper, we apply this approach for the parallelization of a data stream processing application. The description shows the effectiveness of the approach in easily and quickly prototyping several parallel variants of the sequential code by obtaining good overall performance in terms of both throughput and latency.","tags":null,"title":"Data stream processing via code annotations","type":"publication"},{"authors":["Tiziano De Matteis","Gabriele Mencagli"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"76d2e4a6a53e559c434f53286ff123bf","permalink":"/publication/ppopp-2016/","publishdate":"2021-02-18T14:22:23.891779Z","relpermalink":"/publication/ppopp-2016/","section":"publication","summary":"This paper addresses the problem of designing control strategies for elastic stream processing applications. Elasticity allows applications to rapidly change their configuration (e.g. the number of used resources) on-the-fly, in response to fluctuations of their workload. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction horizon by solving an online optimization problem. Our control strategies are designed to address latency constraints, by using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) function of modern multi-core CPUs. The proactive capabilities, in addition to the latency- and energy-awareness, represent the novel features of our approach. Experiments performed using a high-frequency trading application show the effectiveness compared with state-of-the-art techniques.","tags":null,"title":"Keep Calm and React with Foresight: Strategies for Low-Latency and Energy-Efficient Elastic Data Stream Processing","type":"publication"},{"authors":["Tiziano De Matteis"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"cb87785293755b4d16cf3fb2bb151869","permalink":"/publication/dematteis-phd-16/","publishdate":"2021-02-18T14:22:23.892753Z","relpermalink":"/publication/dematteis-phd-16/","section":"publication","summary":"","tags":null,"title":"Parallel Patterns for Adaptive Data Stream Processing","type":"publication"},{"authors":["Tiziano De Matteis","Salvatore Di Girolamo","Gabriele Mencagli"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"48344f056b6b52153fbea7cf06784d03","permalink":"/publication/europar-2015/","publishdate":"2021-02-18T14:22:23.884048Z","relpermalink":"/publication/europar-2015/","section":"publication","summary":"","tags":null,"title":"A Multicore Parallelization of Continuous Skyline Queries on Data Streams","type":"publication"},{"authors":["Marco Danelutto","Tiziano De Matteis","Gabriele Mencagli","Massimo Torquati"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"759f1a5e0f2c34d3a2029f422b406328","permalink":"/publication/repara-2015/","publishdate":"2021-02-18T14:22:23.884645Z","relpermalink":"/publication/repara-2015/","section":"publication","summary":"","tags":null,"title":"Parallelizing High-Frequency Trading Applications by using C+ + 11 Attributes","type":"publication"},{"authors":["Tiziano De Matteis"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"18ec40aba8c43c1b77de869f45bfb60d","permalink":"/publication/hpcs-2014/","publishdate":"2021-02-18T14:22:23.886394Z","relpermalink":"/publication/hpcs-2014/","section":"publication","summary":"","tags":null,"title":"Autonomic Parallel Data Stream Processing","type":"publication"},{"authors":["Daniele Buono","Tiziano De Matteis","Gabriele Mencagli"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"eb2f485782cfdd5382fae6b52504eb3c","permalink":"/publication/ispa-2014/","publishdate":"2021-02-18T14:22:23.885783Z","relpermalink":"/publication/ispa-2014/","section":"publication","summary":"","tags":null,"title":"A High-Throughput and Low-Latency Parallelization of Window-based Stream Joins on Multicores","type":"publication"},{"authors":["Daniele Buono","Marco Danelutto","Tiziano De Matteis","Gabriele Mencagli","Massimo Torquati"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"1b16785c4fe9d08db7333227e9218fbb","permalink":"/publication/pdcn-2014/","publishdate":"2021-02-18T14:22:23.887879Z","relpermalink":"/publication/pdcn-2014/","section":"publication","summary":"The work proposes ffMDF, a lightweight dynamic run-time support able to achieve high performance in the execution of dense linear algebra kernels on shared-cache multi-core. ffMDF implements a dynamic macro-dataflow interpreter processing DAG graphs generated on-the-fly out of standard numeric kernel code. The experimental results demonstrate that the performance obtained using ffMDF on both fine-grain and coarse-grain problems is comparable with or even better than that achieved by de-facto standard solutions (notably PLASMA library), which use separate run-time supports specifically optimised for different computational grains on modern multi-core.","tags":null,"title":"A Lightweight Run-Time Support for Fast Dense Linear Algebra on Multi-Core","type":"publication"},{"authors":["Daniele Buono","Tiziano De Matteis","Gabriele Mencagli","Marco Vanneschi"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"18a8ba9190fd03534c9e256cf5c1d3ee","permalink":"/publication/pdp-2014/","publishdate":"2021-02-18T14:22:23.885277Z","relpermalink":"/publication/pdp-2014/","section":"publication","summary":"","tags":null,"title":"Optimizing Message-Passing on Multicore Architectures Using Hardware Multi-threading","type":"publication"},{"authors":["Tiziano De Matteis","Fabio Luporini","Gabriele Mencagli","Marco Vanneschi"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"27a9d42d328121561df567ae84be386b","permalink":"/publication/pdcn-2013/","publishdate":"2021-02-18T14:22:23.886961Z","relpermalink":"/publication/pdcn-2013/","section":"publication","summary":"The advent of multi-/many-core architectures demands efficient run-time supports to sustain parallel applications scalability. Synchronization mechanisms should be optimized in order to account for different scenarios, such as the interaction between threads executed on different cores as well as intra-core synchronization, i.e. involving threads executed on hardware contexts of the same core. In this perspective, we describe the design issues of two notable mechanisms for shared-memory parallel computations. We point out how specific architectural supports, like hardware cache coherence and core-to-core interconnection networks, make it possible to design optimized implementations of such mechanisms. In this paper we discuss experimental results on three representative architectures: a flagship Intel multi-core and two interesting network processors. The final result helps to untangle the complex implementation space of synchronization mechanisms.","tags":null,"title":"Evaluation of Architectural Supports for Fine-Grained Synchronization Mechanisms","type":"publication"}]